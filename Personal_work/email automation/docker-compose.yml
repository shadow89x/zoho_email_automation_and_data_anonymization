# =====================================================
# DOCKER COMPOSE CONFIGURATION
# Email-Customer Analytics Pipeline Infrastructure
# =====================================================

# What: Complete containerized infrastructure for email analytics pipeline
# Why: Ensures consistent development/production environment across different systems
# How: Uses Docker Compose to orchestrate PostgreSQL database and optional services
# Alternative: Could use cloud databases (AWS RDS, Google Cloud SQL), but local setup is more flexible

version: '3.8'

# =====================================================
# SERVICES CONFIGURATION
# =====================================================

services:
  # PostgreSQL Database Service
  # What: Primary database for storing customer, email, and analytics data
  # Why: PostgreSQL chosen for advanced features (JSON, full-text search, performance)
  # How: Uses official PostgreSQL Docker image with persistent storage
  # Alternative: MySQL (simpler), MongoDB (NoSQL), but PostgreSQL offers best feature set
  postgres:
    image: postgres:15-alpine              # Alpine for smaller image size and security
    container_name: optical-analytics-db   # Descriptive container name
    restart: unless-stopped                # Restart policy for production reliability
    
    # Environment Configuration
    # What: Database connection parameters and initial setup
    # Why: Configures database with secure defaults and proper encoding
    # How: Uses environment variables for flexible configuration
    environment:
      # Database Authentication
      POSTGRES_USER: ${POSTGRES_USER:-optical_admin}           # Admin username (configurable)
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-secure_pass_123} # Admin password (use .env file)
      POSTGRES_DB: ${POSTGRES_DB:-optical_analytics}           # Default database name
      
      # Database Configuration
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=en_US.UTF8"  # Unicode support for international data
      POSTGRES_HOST_AUTH_METHOD: md5                                # Secure authentication method
      
      # Performance Tuning
      POSTGRES_SHARED_PRELOAD_LIBRARIES: pg_stat_statements        # Query performance monitoring
      
    # Port Configuration
    # What: Database connection endpoint
    # Why: Standard PostgreSQL port for easy connection
    # How: Maps container port to host port
    ports:
      - "${POSTGRES_PORT:-5432}:5432"     # Configurable port mapping
    
    # Volume Configuration
    # What: Persistent data storage for database
    # Why: Preserves data across container restarts and updates
    # How: Named volume for better management and backup
    volumes:
      - postgres_data:/var/lib/postgresql/data                    # Main database storage
      - ./sql/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro  # Auto-load schema
      - ./sql/init-data.sql:/docker-entrypoint-initdb.d/02-data.sql:ro # Auto-load initial data (if exists)
      - postgres_logs:/var/log/postgresql                         # Log storage for debugging
    
    # Network Configuration
    # What: Internal network for service communication
    # Why: Isolates database traffic and enables service discovery
    # How: Custom network with predictable naming
    networks:
      - optical_network
    
    # Health Check Configuration
    # What: Container health monitoring
    # Why: Ensures database is ready before other services start
    # How: Uses pg_isready utility for connection testing
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-optical_admin} -d ${POSTGRES_DB:-optical_analytics}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource Limits
    # What: Container resource constraints
    # Why: Prevents database from consuming all system resources
    # How: Sets memory and CPU limits appropriate for development
    deploy:
      resources:
        limits:
          memory: 2G          # Maximum memory usage
          cpus: '1.0'         # Maximum CPU usage
        reservations:
          memory: 512M        # Guaranteed memory
          cpus: '0.5'         # Guaranteed CPU

  # Optional: Redis Cache Service
  # What: In-memory cache for session data and query results
  # Why: Improves application performance for repeated queries
  # How: Uses Redis official image with persistence enabled
  # Alternative: Memcached (simpler), but Redis offers more features
  redis:
    image: redis:7-alpine
    container_name: optical-analytics-cache
    restart: unless-stopped
    
    # Redis Configuration
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    
    ports:
      - "${REDIS_PORT:-6379}:6379"
    
    volumes:
      - redis_data:/data
      - ./config/redis.conf:/usr/local/etc/redis/redis.conf:ro  # Custom configuration (if exists)
    
    networks:
      - optical_network
    
    # Health Check
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    
    # Resource Limits
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'

  # Optional: Jupyter Lab Service
  # What: Interactive development environment for data analysis
  # Why: Provides web-based notebook interface for pipeline development
  # How: Custom image with all dependencies pre-installed
  # Alternative: Local Jupyter installation, but containerized is more consistent
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    container_name: optical-analytics-jupyter
    restart: unless-stopped
    
    ports:
      - "${JUPYTER_PORT:-8888}:8888"
    
    volumes:
      - .:/workspace                        # Mount project directory
      - jupyter_data:/home/jovyan/work      # Persistent workspace
    
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: ${JUPYTER_TOKEN:-optical_analytics_token}
      
      # Database connection for notebooks
      DATABASE_URL: postgresql://${POSTGRES_USER:-optical_admin}:${POSTGRES_PASSWORD:-secure_pass_123}@postgres:5432/${POSTGRES_DB:-optical_analytics}
      REDIS_URL: redis://redis:6379/0
    
    networks:
      - optical_network
    
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Optional: pgAdmin Database Management
  # What: Web-based PostgreSQL administration tool
  # Why: Provides GUI for database management and query development
  # How: Official pgAdmin Docker image with auto-configuration
  # Alternative: Command-line tools (psql), but GUI is more user-friendly
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: optical-analytics-pgadmin
    restart: unless-stopped
    
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@optical.local}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin_password}
      PGADMIN_CONFIG_SERVER_MODE: 'False'
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: 'False'
    
    ports:
      - "${PGADMIN_PORT:-5050}:80"
    
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./config/servers.json:/pgadmin4/servers.json:ro  # Auto-configure database connection
    
    networks:
      - optical_network
    
    depends_on:
      postgres:
        condition: service_healthy

# =====================================================
# VOLUMES CONFIGURATION
# =====================================================

# What: Persistent storage definitions for data preservation
# Why: Ensures data survives container restarts and updates
# How: Named volumes managed by Docker with backup capabilities
volumes:
  # PostgreSQL Data Storage
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-./data/postgres}  # Configurable data path
  
  # PostgreSQL Logs
  postgres_logs:
    driver: local
  
  # Redis Data Storage
  redis_data:
    driver: local
  
  # Jupyter Workspace
  jupyter_data:
    driver: local
  
  # pgAdmin Configuration
  pgadmin_data:
    driver: local

# =====================================================
# NETWORKS CONFIGURATION
# =====================================================

# What: Custom network for service communication
# Why: Provides isolated network with service discovery
# How: Bridge network with custom subnet for predictable addressing
networks:
  optical_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16  # Custom subnet to avoid conflicts

# =====================================================
# CONFIGURATION NOTES
# =====================================================

# Environment Variables:
# Create a .env file in the project root with the following variables:
# 
# # Database Configuration
# POSTGRES_USER=optical_admin
# POSTGRES_PASSWORD=your_secure_password_here
# POSTGRES_DB=optical_analytics
# POSTGRES_PORT=5432
# 
# # Redis Configuration
# REDIS_PORT=6379
# 
# # Jupyter Configuration
# JUPYTER_PORT=8888
# JUPYTER_TOKEN=your_jupyter_token_here
# 
# # pgAdmin Configuration
# PGADMIN_EMAIL=admin@yourcompany.com
# PGADMIN_PASSWORD=your_pgadmin_password
# PGADMIN_PORT=5050
# 
# # Data Storage
# DATA_PATH=./data/postgres

# Security Considerations:
# 1. Change default passwords in production
# 2. Use secrets management for sensitive data
# 3. Enable SSL/TLS for database connections
# 4. Restrict network access with firewall rules
# 5. Regular security updates for container images

# Performance Tuning:
# 1. Adjust PostgreSQL configuration for your workload
# 2. Monitor resource usage and adjust limits
# 3. Use SSD storage for better I/O performance
# 4. Consider read replicas for high-read workloads

# Backup Strategy:
# 1. Regular PostgreSQL dumps: pg_dump optical_analytics
# 2. Volume snapshots for point-in-time recovery
# 3. Offsite backup storage for disaster recovery
# 4. Test restore procedures regularly

# Development vs Production:
# Development: Use this configuration as-is
# Production: Consider managed database services, load balancers, monitoring