# üöÄ Complete Setup Guide
## Email-Customer Analytics Pipeline

### üìã Prerequisites

1. **Python 3.11+** installed
2. **Docker and Docker Compose** (for database)
3. **Git** for version control
4. **Zoho API Account** (for email retrieval)

---

## üîß Step 1: Environment Setup

### Create Virtual Environment
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Upgrade pip
python -m pip install --upgrade pip
```

### Install Dependencies
```bash
# Install all required packages
pip install -r requirements.txt

# Install spaCy model for NLP (required for Presidio)
python -m spacy download en_core_web_lg
```

---

## üîê Step 2: Credentials Configuration

### Create credentials.json file
```bash
# Create the credentials file
touch credentials.json
```

### Edit credentials.json with your actual values:
```json
{
  "zoho": {
    "client_id": "1000.YOUR_ACTUAL_CLIENT_ID",
    "client_secret": "your_actual_client_secret_here",
    "refresh_token": "1000.your_actual_refresh_token_here"
  },
  "database": {
    "host": "localhost",
    "port": 5432,
    "database": "optical_analytics",
    "username": "optical_admin",
    "password": "your_secure_password_123"
  }
}
```

### üîë How to Get Zoho API Credentials

1. **Go to Zoho API Console**: https://api-console.zoho.com/
2. **Create New Application**:
   - Application Type: `Server-based Applications`
   - Application Name: `Email Analytics Pipeline`
3. **Configure OAuth**:
   - Authorized Redirect URIs: `http://localhost:8080/callback`
   - Scopes: `ZohoMail.messages.READ`
4. **Get Authorization Code**:
   ```
   https://accounts.zoho.com/oauth/v2/auth?scope=ZohoMail.messages.READ&client_id=YOUR_CLIENT_ID&response_type=code&redirect_uri=http://localhost:8080/callback&access_type=offline
   ```
5. **Exchange for Refresh Token**:
   ```bash
   curl -X POST https://accounts.zoho.com/oauth/v2/token \
     -d "grant_type=authorization_code" \
     -d "client_id=YOUR_CLIENT_ID" \
     -d "client_secret=YOUR_CLIENT_SECRET" \
     -d "redirect_uri=http://localhost:8080/callback" \
     -d "code=YOUR_AUTHORIZATION_CODE"
   ```

---

## üê≥ Step 3: Docker Environment Setup

### Create .env file for Docker
```bash
# Create environment file
touch .env
```

### Add to .env file:
```bash
# Database Configuration
POSTGRES_USER=optical_admin
POSTGRES_PASSWORD=secure_password_123
POSTGRES_DB=optical_analytics
POSTGRES_PORT=5432

# Redis Configuration  
REDIS_PORT=6379

# Jupyter Configuration
JUPYTER_PORT=8888
JUPYTER_TOKEN=optical_analytics_token_123

# pgAdmin Configuration
PGADMIN_EMAIL=admin@yourcompany.com
PGADMIN_PASSWORD=admin_password_123
PGADMIN_PORT=5050

# Data Storage
DATA_PATH=./data/postgres
```

### Start Docker Services
```bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f postgres
```

---

## üìä Step 4: Database Setup

### Initialize Database Schema
```bash
# Connect to database and run schema
docker-compose exec postgres psql -U optical_admin -d optical_analytics -f /docker-entrypoint-initdb.d/01-schema.sql
```

### Verify Database Connection
```bash
# Test connection
docker-compose exec postgres psql -U optical_admin -d optical_analytics -c "\dt"
```

---

## üìÅ Step 5: Data Directory Setup

### Create Required Directories
```bash
# Create data directories
mkdir -p data/raw
mkdir -p data/processed  
mkdir -p data/exports
mkdir -p data/backups
mkdir -p chunk
mkdir -p logs

# Set permissions (Linux/macOS)
chmod 755 data/
chmod 755 chunk/
chmod 755 logs/
```

### Place Your Data Files
```
data/raw/
‚îú‚îÄ‚îÄ cc (1).csv          # Customer data
‚îú‚îÄ‚îÄ fi.CSV              # Inventory data
‚îú‚îÄ‚îÄ s_by_c.CSV          # Sales data
‚îî‚îÄ‚îÄ zoho_emails.json    # Email data (generated by pipeline)
```

---

## üß™ Step 6: Test Configuration

### Test Zoho API Connection
```bash
# Run the email retrieval test
cd notebooks/
jupyter lab
# Open: automated_zoho_email_retrieval_pipeline.ipynb
# Run the authentication test cell
```

### Test Database Connection
```bash
# Test database connectivity
python -c "
import psycopg2
try:
    conn = psycopg2.connect(
        host='localhost',
        port=5432,
        database='optical_analytics',
        user='optical_admin',
        password='secure_password_123'
    )
    print('‚úÖ Database connection successful!')
    conn.close()
except Exception as e:
    print(f'‚ùå Database connection failed: {e}')
"
```

### Test Data Processing
```bash
# Run basic data processing test
python src/data_utils.py
```

---

## üöÄ Step 7: Run the Pipeline

### Option A: Interactive Notebooks (Recommended)
```bash
# Start Jupyter Lab
jupyter lab

# Follow notebooks in order:
# 1. automated_zoho_email_retrieval_pipeline.ipynb
# 2. comprehensive_data_analysis_cleaning_pipeline.ipynb  
# 3. advanced_customer_email_matching_enrichment_pipeline.ipynb
# 4. customer_email_data_anonymization_pipeline.ipynb
```

### Option B: Command Line Execution
```bash
# Run core processing pipeline
python src/email_customer_matching.py
python src/business_id_mapping.py
python src/check_customer_matching.py
```

---

## üìà Step 8: Access Tools

### Jupyter Lab (Data Analysis)
- **URL**: http://localhost:8888
- **Token**: `optical_analytics_token_123` (from .env)

### pgAdmin (Database Management)
- **URL**: http://localhost:5050
- **Email**: admin@yourcompany.com
- **Password**: admin_password_123

### Database Direct Connection
```bash
# Command line access
docker-compose exec postgres psql -U optical_admin -d optical_analytics
```

---

## üîç Step 9: Verification Checklist

### ‚úÖ Environment Verification
- [ ] Python 3.11+ installed and activated
- [ ] All packages installed successfully
- [ ] spaCy model downloaded
- [ ] credentials.json created with real values
- [ ] .env file created for Docker

### ‚úÖ Services Verification  
- [ ] Docker containers running (postgres, redis, jupyter, pgadmin)
- [ ] Database schema created successfully
- [ ] Jupyter Lab accessible
- [ ] pgAdmin accessible
- [ ] Data directories created

### ‚úÖ Pipeline Verification
- [ ] Zoho API authentication working
- [ ] Database connection established
- [ ] Sample data processing successful
- [ ] Notebooks running without errors

---

## üõ†Ô∏è Troubleshooting

### Common Issues

#### 1. **Zoho API Authentication Fails**
```bash
# Check credentials format
cat credentials.json | python -m json.tool

# Verify API permissions
# Ensure scopes include: ZohoMail.messages.READ
```

#### 2. **Database Connection Issues**
```bash
# Check Docker services
docker-compose ps

# Restart database
docker-compose restart postgres

# Check logs
docker-compose logs postgres
```

#### 3. **Permission Errors**
```bash
# Fix data directory permissions (Linux/macOS)
sudo chown -R $USER:$USER data/
sudo chown -R $USER:$USER chunk/
sudo chown -R $USER:$USER logs/
```

#### 4. **Package Installation Issues**
```bash
# Update pip and setuptools
pip install --upgrade pip setuptools wheel

# Install with verbose output
pip install -r requirements.txt -v

# Clear pip cache if needed
pip cache purge
```

### Getting Help

1. **Check logs** in the `logs/` directory
2. **Review error messages** carefully
3. **Check GitHub issues** for similar problems
4. **Verify all prerequisites** are met
5. **Test each component** individually

---

## üéØ Next Steps After Setup

1. **Data Quality Assessment**: Run data analysis notebooks
2. **Customer Matching**: Execute matching algorithms
3. **Business Intelligence**: Generate insights and reports
4. **Monitoring Setup**: Configure logging and alerting
5. **Backup Strategy**: Implement regular data backups

---

## üîí Security Best Practices

1. **Never commit credentials** to version control
2. **Use strong passwords** for all services
3. **Enable 2FA** where possible
4. **Regular credential rotation**
5. **Monitor access logs**
6. **Keep dependencies updated**

---

*Setup complete! You're now ready to run the email-customer analytics pipeline! üéâ* 